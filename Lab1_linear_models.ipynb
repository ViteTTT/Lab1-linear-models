{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lab 1: Linear models - Part 1: Regression\n",
    "\n",
    "# Auto-setup when running on Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install openml\n",
    "\n",
    "# General imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "from matplotlib import cm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Hide convergence warning for now\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Hiding all warnings. Not recommended, just for compilation.\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# Download NO2 data. Takes a while the first time.\n",
    "no2 = oml.datasets.get_dataset(547)\n",
    "X, y, _, _ = no2.get_data(target=no2.default_target_attribute)\n",
    "attribute_names = list(X)\n",
    "\n",
    "# Quick visualization\n",
    "df = pd.DataFrame(X, columns=attribute_names).join(pd.DataFrame(list(y),columns=['target']))\n",
    "df = df.sort_values(['day','hour_of_day']).drop('day',axis=1)\n",
    "df.plot(use_index=False,figsize=(20,5),cmap=cm.get_cmap('brg'));\n",
    "X = X.drop('day',axis=1)\n",
    "df.head()\n",
    "\n",
    "# Drop wind_direction for a better plot\n",
    "df.drop('wind_direction',axis=1).plot(use_index=False,figsize=(20,5),cmap=cm.get_cmap('brg'));\n",
    "\n",
    "# Exercise 1.1: Model benchmark\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_learners(models, X, y):\n",
    "    # Evaluate each model in 'models' with cross-validation on the provided (X, y) data.\n",
    "    xvals = [cross_validate(m, X, y, return_train_score=True, n_jobs=-1) for m in models]\n",
    "    test_scores = [x['test_score'] for x in xvals]\n",
    "    train_scores = [x['train_score'] for x in xvals]\n",
    "    return train_scores, test_scores\n",
    "\n",
    "# Exercise 1.2: Run evaluation\n",
    "models = [LinearRegression(), Ridge(), Lasso(), ElasticNet(), KNeighborsRegressor()]\n",
    "train_scores, test_scores = evaluate_learners(models, X, y)\n",
    "\n",
    "# Plot train/test scores\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "width=0.45\n",
    "ax.barh(np.arange(len(train_scores)), np.mean(test_scores, axis=1), width,\n",
    "        yerr= np.std(test_scores, axis=1), color='green', label='test R^2')\n",
    "ax.barh(np.arange(len(train_scores))-width, np.mean(train_scores, axis=1), width,\n",
    "        yerr= np.std(train_scores, axis=1), color='red', label='train R^2')\n",
    "for i, te, tr in zip(np.arange(len(train_scores)),test_scores,train_scores):\n",
    "    ax.text(0, i, \"{:.3f} +- {:.3f}\".format(np.mean(te),np.std(te)), \n",
    "            color=('white' if np.mean(te)>0.1 else 'black'), va='center')\n",
    "    ax.text(0, i-width, \"{:.3f} +- {:.3f}\".format(np.mean(tr),np.std(tr)), \n",
    "            color=('white' if np.mean(tr)>0.1 else 'black'), va='center')\n",
    "labels = [c.__class__.__name__ if not hasattr(c, 'steps') else c.steps[0][0] + \"_\" + c.steps[1][0] for c in models]\n",
    "ax.set(yticks=np.arange(len(train_scores))-width/2, yticklabels=labels)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.show()\n",
    "\n",
    "# Exercise 2: Regularization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_a = {'alpha': np.logspace(-12, 12, num=22)}\n",
    "param_elastic = {'l1_ratio': np.linspace(0, 1, num=11),\n",
    "                 'alpha': np.logspace(-12, 12, num=25)}\n",
    "param_k = {'kneighborsregressor__n_neighbors': np.geomspace(1, 60, num=12, dtype=int)[1:]}\n",
    "\n",
    "models = [Ridge(), Lasso(), make_pipeline(StandardScaler(), KNeighborsRegressor()), ElasticNet()]\n",
    "grids = [param_a, param_a, param_k, param_elastic]\n",
    "\n",
    "# Helper function for plotting tuning results\n",
    "def plot_tuning(grid_search, param_name, ax):\n",
    "    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_test_score'], marker='.', label='Test score')\n",
    "    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_train_score'], marker='.', label='Train score')\n",
    "    ax.set_ylabel('R^2 score')\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title(grid_search.best_estimator_.__class__.__name__)\n",
    "    bp, bs = grid_search.best_params_[param_name], grid_search.best_score_\n",
    "    ax.text(bp, bs+0.01, f\"best:{bp}, R2:{bs:.4f}\")\n",
    "\n",
    "grid_searches = [GridSearchCV(m,grid,n_jobs=-1, cv=3, return_train_score=True).fit(X,y) for m,grid in zip(models,grids)]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "for grid_search, param, ax in zip(grid_searches[0:3], ['alpha','alpha','kneighborsregressor__n_neighbors'], axes):\n",
    "    plot_tuning(grid_search, param, ax)\n",
    "plt.show()\n",
    "\n",
    "# Exercise 2.3: ElasticNet grid search heatmap\n",
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\", printvalues=False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel, fontsize=10)\n",
    "    ax.set_ylabel(ylabel, fontsize=10)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12, labelrotation=90)\n",
    "    if(printvalues):\n",
    "        for p, color, value in zip(img.get_paths(), img.get_facecolors(), img.get_array()):\n",
    "            x, y = p.vertices[:-2, :].mean(0)\n",
    "            c = 'k' if np.mean(color[:3]) > 0.5 else 'w'\n",
    "            ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\", size=10)\n",
    "    return img\n",
    "\n",
    "scores = np.array(pd.DataFrame(grid_searches[3].cv_results_).mean_test_score).reshape(25, 11).T\n",
    "fig, axes = plt.subplots(1, 1, figsize=(13, 13))\n",
    "heatmap(scores, xlabel='alpha', xticklabels=list(map(lambda n: \"%.E\" % n, param_elastic['alpha'])),\n",
    "        ylabel='l1_ratio', yticklabels=np.around(param_elastic['l1_ratio'],4), cmap=\"viridis\", fmt=\"%.2f\", ax=axes)\n",
    "plt.show()\n",
    "\n",
    "# Exercise 3: Visualizing coefficients\n",
    "def scatter_coefficients(alpha=0.001):\n",
    "    models = [LinearRegression(), Ridge(alpha=alpha), Lasso(alpha=alpha), ElasticNet(alpha=alpha)]\n",
    "    coeff = [m.fit(X,y).coef_ for m in models]\n",
    "    attribute_names = list(X)\n",
    "    col = ['k','b','r','y']\n",
    "    plt.figure()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    for i in range(0,4):\n",
    "        plt.scatter(attribute_names, coeff[i], s=(4-i)*40, c=col[i], label=models[i].__class__.__name__)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "scatter_coefficients(alpha=0.001)\n",
    "scatter_coefficients(alpha=1)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
